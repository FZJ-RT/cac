{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note:\n",
    "#In this tutorial you will learn how to use CAC Python library for image segmentation using U-Net\n",
    "#We provide data consisting of 3 original images and 3 binary labels with the size 512 x 512 pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import external libraries\n",
    "import numpy\n",
    "import os\n",
    "import shutil\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import CAC python library\n",
    "from cac import model, trainer, utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before creating training and test set, we need to check whether both our image and label have the same size\n",
    "#\n",
    "#For checking the image, we use this function: utilities.read_single_image(filename=None, forced_size=None, scaled=False, info=False)\n",
    "#filename refers to the image file that you want to load, the supported format follows the current opencv capabilities\n",
    "#forced_size is to resize the loaded image using opencv tool\n",
    "#scaled is to scale the value of the image to 0 - 1\n",
    "#info is to provide any information regarding the resizing result\n",
    "#\n",
    "#You can also use this function for later prediction or visualization \n",
    "#Since we do not need to change the size, we set forced_size=None\n",
    "checked_image = utilities.read_single_image('data/images/0.png', forced_size=None, scaled=False, info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For checking the label, we use this function: utilities.read_single_label(filename=None, forced_size=None, scaled=False, info=False)\n",
    "#filename refers to the label file that you want to load, the supported format follows the current opencv capabilities\n",
    "#forced_size is to resize the loaded label using opencv tool\n",
    "#scaled is to scale the value of the image to 0 - 1\n",
    "#info is to provide any information regarding the resizing result\n",
    "#\n",
    "#You can also use this function for later prediction or visualization \n",
    "#Since we do not need to change the size, we set forced_size=None\n",
    "checked_label = utilities.read_single_label('data/labels/0.png', forced_size=None, scaled=False, info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Once we are sure with the quality of our images and labels, we then create an image and label pool\n",
    "#\n",
    "#For creating image pool, we use this function: utilities.read_images_from_folder(folder_dir=None, forced_size=None, scaled=True)\n",
    "#folder_dir refers to folder containing all images for training, we recommend to use enumeration to name your images\n",
    "#forced_size is to resize the loaded image using opencv tool\n",
    "#scaled is to scale the value of the image to 0 - 1\n",
    "#\n",
    "#We need to change the size to reduce the computational burden later, here we use 64 x 64 just for tutorial purpose!!!\n",
    "#Remember, the size of the image should be a factor of 2\n",
    "image_pool    = utilities.read_images_from_folder('data/images', forced_size=(64, 64), scaled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For creating label pool, we use this function: utilities.read_labels_from_folder(folder_dir=None, forced_size=None, scaled=True)\n",
    "#folder_dir refers to folder containing all labels for training, we recommend to use enumeration to name your labels\n",
    "#forced_size is to resize the loaded label using opencv tool\n",
    "#scaled is to scale the value of the image to 0 - 1\n",
    "#\n",
    "#We need to change the size to reduce the computational burden later, here we use 64 x 64 just for tutorial purpose!!!\n",
    "#Remember, the size of the label should be a factor of 2\n",
    "label_pool    = utilities.read_labels_from_folder('data/labels', forced_size=(64, 64), scaled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are now ready to create our training, test, and prediction set\n",
    "#\n",
    "#For this purpose, we will use this function: utilities.create_set(pool_image_array=None, pool_label_array=None, no_training_samples=None, no_test_samples=None, documentation_path=None, random_seed=1234567)\n",
    "#pool_image_array refers to image pool variable we created before --> image_pool\n",
    "#pool_label_array refers to array pool variable we created before --> label_pool\n",
    "#no_training_samples is to determine the amount of training samples\n",
    "#no_test_samples is to determine the amount of test samples \n",
    "#This function will produce excel file, containing details of images going to training, test, or prediction sample\n",
    "#random_seed is to ensure reproducibility\n",
    "#\n",
    "#This function will randomly split the images to  training, test, and validation set. If you only fill \"no_training_samples\", you will only get training and test set\n",
    "#If you fill both \"no_training_samples\" and \"no_test_samples\", you will create training, test, and prediction set\n",
    "#Note that, the number of training samples + test samples + prediction samples = your total images\n",
    "#\n",
    "#For the current tutorial, we want to put 6 images as training samples, 2 images as test samples, and 1 image as prediction sample\n",
    "training_images, training_labels, \\\n",
    "test_images, test_labels, \\\n",
    "prediction_images, prediction_labels = utilities.create_set(pool_image_array=image_pool, pool_label_array=label_pool,\n",
    "                                                no_training_samples=1, no_test_samples=1,\n",
    "                                                documentation_path='data') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After preparing the training, test, and prediction samples, we are now ready for creating our U-Net model\n",
    "#In CAC Python library, we only provide built-in vanilla U-Net architecture\n",
    "#If user wants to go with different architecture, user can replace this code section with any Tensorflow architecture\n",
    "#\n",
    "#For defining a new U-Net model, we use this function: model.create_vanilla_unet(training_images, training_labels, encoder_depth=5, number_filters_at_first_encoder=64, \n",
    "#pooling_size=(2,2), kernel_size=(3,3), batchnorm_axis=-1, learning_rate=1E-3, distributed=False, info=False)\n",
    "#This function allows use to define you own pooling, kernel size, batchnorm, and learning rate etc.\n",
    "#We recommend to leave the pooling_size, kernel_size, and batchnorm axis untouched !!!\n",
    "#If you install the GPU version of this library, you can set \"distributed=True\" which will activate Horovod library to allow training with GPU\n",
    "#Without installing the GPU version, setting \"distributed=True\" will give you an error\n",
    "#\"info=True\" will give you a summary of the architecture\n",
    "#Make sure you set a variable as an output of this function --> you will pass this variable to the trainer\n",
    "unet = model.create_vanilla_unet(training_images, training_labels, \n",
    "                                 encoder_depth=4,\n",
    "                                 number_filters_at_first_encoder=64,\n",
    "                                 learning_rate=1E-3, \n",
    "                                 distributed=False,\n",
    "                                 info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Once you defined a model, we will proceed to tha training\n",
    "#\n",
    "#For this purpose, we use this function: trainer.train_with_generator(training_images, training_labels, model, batch_size, epochs, distributed=False, save_path=None)\n",
    "#Here, you can modify the batch size and epochs\n",
    "#If you use train_with_generator, the library will internally create a generator for you which will help you to perform augmentation\n",
    "#If you install the GPU version of this library, you can set \"distributed=True\" which will activate Horovod library to allow training with GPU\n",
    "#Without installing the GPU version, setting \"distributed=True\" will give you an error\n",
    "#save_path is for saving the trained model\n",
    "trained_unet = trainer.train_with_generator(training_images, training_labels,\n",
    "                                            unet, batch_size=1, epochs=10,\n",
    "                                            distributed=False,\n",
    "                                            save_path='results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This section is to help you select a good hyperparameters\n",
    "#It is based on accuracy in predicting the test set\n",
    "#\n",
    "#For this purpose we use this function: trainer.calculate_image_metric(test_images, test_labels, model, save_path=None)\n",
    "#save_path is for saving the test metric --> it will produce a text file with the name \"test_error.txt\"\n",
    "#Note that, the error is very high. It is only for tutorial purpose.\n",
    "test_error = trainer.calculate_image_metric(test_images, test_labels,\n",
    "                                            trained_unet,\n",
    "                                            save_path='results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Once all the training is done, we can use our model to make prediction\n",
    "#\n",
    "#For this purpose, we use this function: trainer.calculate_jaccard_score(test_images, test_labels, model, reference_label=None, save_path=None)\n",
    "#save_path is for saving the prediction metric --> it will produce a text file with the name \"jaccard.txt\"\n",
    "#If you put reference_label here, it will superpose your prediction result with the reference label (knowing that the geometry of the chip stays the same)\n",
    "#If you do not have any prediction label for the accurayc calculation, you can skip this section\n",
    "prediction_error = trainer.calculate_jaccard_score(prediction_images, prediction_labels,\n",
    "                                                   trained_unet,\n",
    "                                                   save_path='results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
